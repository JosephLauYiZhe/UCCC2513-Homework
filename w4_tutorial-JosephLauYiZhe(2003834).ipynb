{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c00aa5-19d1-4ab3-8ca1-02f3ba5eb324",
   "metadata": {},
   "source": [
    "# Tutorial week 4 \n",
    "\n",
    "# Learning outcomes\n",
    "1. Gentle introduction to what is computer vision\n",
    "2. Review: Important concepts of image stored as Numpy array\n",
    "3. Image cropping\n",
    "4. Different types of color models: HSV, RGB and CIELAB\n",
    "    * Split and merge image channels\n",
    "    * Manipulate the image channels\n",
    "5. Point operators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caba8fd3-83ac-4a95-884e-aad7c124791d",
   "metadata": {},
   "source": [
    "# What is digital image processing / computer vision?\n",
    "\n",
    "As humans, we perceive the 3D structure of the world around us with ease. For example, looking at a framed group portrait, you can easily count and name all the people in the picture and even guess at their emotions from their facial expressions.\n",
    "\n",
    "Perceptual psychologists have spent decades trying to comprehend how visual system works and optical illusions have been discovered to solve the puzzle, a complete solution is still far beyond our reach. \n",
    "\n",
    "Computer vision / digital image processing is being utilized in diverse of real world applications:\n",
    "- Optical character recognition (OCR): reading handwritten postal codes on letters and automatic plate recognition.\n",
    "\n",
    "  ![OCR](https://upload.wikimedia.org/wikipedia/commons/thumb/8/81/Portable_scanner_and_OCR_%28video%29.webm/1200px--Portable_scanner_and_OCR_%28video%29.webm.jpg \"Optical character recognition\")\n",
    "- Medical imaging: registering pre-operative and intra-operative imagery or performing long term studies of internal organ.\n",
    "\n",
    "  ![CT scan](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRta2V0mlQ-4oVHOUfyhRGpyPm64T4smphtzg&s \"CT scans\")\n",
    "- Self-driving vehicles.\n",
    "\n",
    "  ![Autonomous driving](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQdDERoAxq7k7ujdso6ghyI2hm6yn2cK9pJAQ&s \"autonomous vehicle\")\n",
    "- Surveillance: monitoring for intruders, analyzing highway traffic and monitoring pools for drowning victims.\n",
    "\n",
    "  ![surveillance](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSP9mr_ytNAapxlWafQLG5AcZZKVZ2wgPheFQ&s \"surveillance system\")\n",
    "- Fingerprint recognition and biometrics: automatic access authentication as well as forensic applications.\n",
    "\n",
    "  ![biometrics](https://www.nec.co.nz/wp-content/uploads/2018/02/Close-up-of-womans-left-eye-showing-iris-recogntion-points-market.jpg \"iris recognition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ea280-c656-4771-9f55-c84757ba5b82",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06b14005-c4bd-43e6-b3b1-129b888bed35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "# Python 3.8 is required\n",
    "assert sys.version_info >= (3, 8)\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "#from utils import display_images\n",
    "\n",
    "# Make sure that optimization is enabled\n",
    "if not cv.useOptimized():\n",
    "    cv.setUseOptimized(True)\n",
    "    \n",
    "cv.useOptimized()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa2bd5-33d5-4aa5-9867-071f5abdc953",
   "metadata": {},
   "source": [
    "# Review: Representation of image as Numpy array\n",
    "An image is a multidimensional array; it has columns and rows of pixels, and each pixel has a value. For different kinds of image data, the pixel value may be formatted in different ways. We can create a $4\\times 4$ square black image from scratch by simply creating a 2D NumPy array as shown in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46498af3-72df-4eb9-bb84-c31c850c7b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "img = np.zeros((4, 4), dtype = np.uint8)\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4659d1a7-089d-48e8-b863-e3af23c48d01",
   "metadata": {},
   "source": [
    "Here, each pixel is represented by a single 8-bit integer, which means that the values of each pixel are in 0-255 range, where 0 is black, 255 is white and the in-between values are shades of gray. This is a **grayscale** image. You can use `cv.cvtColor()` to convert the images from one color space to another. We will discuss about image color spaces later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b732522a-9e39-4117-a881-53d912566ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "img_bgr = cv.cvtColor(img, cv.COLOR_GRAY2BGR)\n",
    "print(img_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "861d4c4e-bae0-49be-8a54-626fdc705550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 3)\n"
     ]
    }
   ],
   "source": [
    "print(img_bgr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb35e707-c028-4d0b-8c48-0b0a56804e2d",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "---\n",
    "1. Create a $200 \\times 200$ white image and display it. \n",
    "2. Leverage your image processing skills to create a simple wallpaper design as shown in the following image:\n",
    "\n",
    "![pattern](img_embed/exercise_w4.jpg \"Pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "169a3b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff5206f9070>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#create a patch with alternative black and white\n",
    "\n",
    "tile = np.zeros((60,60),dtype = \"uint8\")\n",
    "\n",
    "#slice subpatch to assign new values \n",
    "tile[:20,20:40] = 255\n",
    "tile[20:40,:20] = 255\n",
    "tile[20:40,40:] = 255\n",
    "tile[40:,20:40] = 255\n",
    "\n",
    "img = np.tile(tile,(3,3))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(img, cmap = \"gray\",vmin = 0, vmax = 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "699f4b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images/flower.jfif\")\n",
    "\n",
    "from utils import display_image\n",
    "display_image(\"image\",img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "128236fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 , 49\n"
     ]
    }
   ],
   "source": [
    "#1st way:GUI(callback)\n",
    "\n",
    "def mouse(event,x,y,flags,params):\n",
    "    if event == cv.EVENT_LBUTTONDOWN: #left mouse click\n",
    "        print(x,\",\", y)\n",
    "        cv.imshow(\"Image\",img)\n",
    "        \n",
    "cv.imshow(\"image\",img)\n",
    "cv.setMouseCallback(\"image\",mouse)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6233ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flower = img[36:112,90:175]\n",
    "display_image(\"flower\",flower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a413b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 128, 0, 0)\n"
     ]
    }
   ],
   "source": [
    "#2nd way:\n",
    "\n",
    "r = cv.selectROI(\"roi\",img)\n",
    "print(r)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96dffe62",
   "metadata": {},
   "source": [
    "the output is $(x,y,w,h)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,w,h = r\n",
    "\n",
    "flower = img[y:y+h,x:x+w]\n",
    "display_image(\"flower\",flower)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3f011",
   "metadata": {},
   "source": [
    "3 ways to extract regions from image\n",
    "\n",
    "1. use callback function(GUI)\n",
    "2. use cv.selectROI()\n",
    "3. use external software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ead7283-fed0-4e83-b4b4-8b580abb2e10",
   "metadata": {},
   "source": [
    "## Access elements in an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f448bb-9739-404b-ba7e-976b538c3a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('images/lena.jfif')\n",
    "\n",
    "%timeit a = img[100, 30, 0]\n",
    "%timeit b = img.item(100, 30, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486084d-a7ef-498d-8317-4d980653d7f5",
   "metadata": {},
   "source": [
    "## Numpy array slicing\n",
    "### Exercise\n",
    "2. Extract the region of interest (flower) from the 'flower.jfif'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1667dbc5-74ae-4618-9434-41a807ecda69",
   "metadata": {},
   "source": [
    "# Cropping an image\n",
    "Why crop an image? \n",
    "1. Remove all unwanted objects or areas from an image\n",
    "2. Improve the overall composition of the image. Visit this [link](https://expertphotography.com/improve-your-composition-the-rule-of-thirds/) on how separating image into grids and putting the subject of interest on the intersection point could create a more compositional pleasing photo. This is known as of rule of thirds.\n",
    "3. One of the image augmentation techniques in deep learning model training.\n",
    "\n",
    "The operations are literally the same as extracting ROI.\n",
    "\n",
    "## Divide an image into smaller patches using cropping\n",
    "One practical application of cropping in OpenCV is to divide an image into smaller patches. The following example shows how to split image into a $2 \\times 3$ grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6415b308-8cff-4444-884c-c304e90d075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread('images/dog.jfif')\n",
    "img_copy = img.copy()\n",
    "\n",
    "height, width = img.shape[:2]\n",
    "num_vertical_patches = 2\n",
    "num_horizontal_patches = 3\n",
    "\n",
    "# M and N are basically number of pixels per patch\n",
    "M, N = int(height / num_vertical_patches), int(width / num_horizontal_patches)\n",
    "\n",
    "x1, y1 = 0, 0\n",
    "\n",
    "for y in range(0, height, M):\n",
    "    for x in range(0, width, N):\n",
    "        \n",
    "        y1 = y + M\n",
    "        x1 = x + N\n",
    "        \n",
    "        if x1>=width and y1>=height:\n",
    "            x1 = width-1\n",
    "            y1 = height-1 #indexing so minus 1 because start with 0 \n",
    "            tile = img[y:height, x:width]\n",
    "            cv.rectangle(img_copy, (x,y), (x1, y1), (0, 255, 0), 1)\n",
    "            cv.imshow('tile', tile)\n",
    "            \n",
    "        elif y1>=height:\n",
    "            y1 = height-1\n",
    "            cv.rectangle(img_copy, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "            \n",
    "        elif x1>=width:\n",
    "            x1 = width-1\n",
    "            cv.rectangle(img_copy, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "            \n",
    "        else:\n",
    "            cv.rectangle(img_copy, (x, y), (x1, y1), (0, 255, 0), 1)\n",
    "\n",
    "cv.imshow('patched image', img_copy)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea736b7-ef45-4d71-bb84-e8f5635a9fd6",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Divide the image into 4 equal regions. Swap their positions as shown below:\n",
    "\n",
    "   ![dog_swap](img_embed/crop_swap.PNG \"dog\")\n",
    "2. Cover the face of lena with white mask as shown as the following:\n",
    "\n",
    "   ![lena_mask](img_embed/lena_mask.PNG \"lena mask\")\n",
    "\n",
    "3. Extract the region of interest (flower) from the 'flower.jfif'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c5b2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question1\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# Load the image\n",
    "img = cv.imread('images/dog.jfif')\n",
    "\n",
    "height, width = img.shape[:2]\n",
    "\n",
    "# Divide the image into 4 regions\n",
    "mid_h, mid_w = height // 2, width // 2\n",
    "top_left = img[0:mid_h, 0:mid_w]\n",
    "top_right = img[0:mid_h, mid_w:width]\n",
    "bottom_left = img[mid_h:height, 0:mid_w]\n",
    "bottom_right = img[mid_h:height, mid_w:width]\n",
    "\n",
    "# Create a new image and swap regions\n",
    "new_image = np.zeros_like(img)\n",
    "new_image[0:mid_h, 0:mid_w] = bottom_right\n",
    "new_image[0:mid_h, mid_w:width] = bottom_left\n",
    "new_image[mid_h:height, 0:mid_w] = top_right\n",
    "new_image[mid_h:height, mid_w:width] = top_left\n",
    "\n",
    "# Display the images\n",
    "cv.imshow('Original Image', img)\n",
    "cv.imshow('swapped regions', new_image)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9839a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question2\n",
    "\n",
    "img = cv.imread('images/lena.jfif')\n",
    "\n",
    "# Get the dimensions of the image\n",
    "roi  = cv.selectROI(img)\n",
    "print(roi)\n",
    "\n",
    "# Draw a white rectangle over the face\n",
    "cv.rectangle(img, (85, 87), (85+75, 87+85), (255, 255, 255), -1)\n",
    "\n",
    "# Display the image with the white mask\n",
    "cv.imshow('Lena with White Mask', img)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ff42c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question3\n",
    "\n",
    "img = cv.imread('images/flower.jfif')\n",
    "x,y,w,h = cv.selectROI(img)\n",
    "flower = img[y:y+h,x:x+w]\n",
    "cv.imshow('Flower', flower)\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e201988-a45e-41a7-80a3-d1d6ad93f1cf",
   "metadata": {},
   "source": [
    "# Color space / color model\n",
    "In the most common color space, RGB (Red Green Blue), colors are represented in terms of their red, green, and blue components. In more technical terms, RGB describes a color as a tuple of 3 components. Each component can take a value between 0 and 255, where the tuple (0, 0, 0) represents black and (255, 255, 255) represents white. RGB is considered an \"additive\" color space, and colors can be imagined as being produced from shining quantities of red, blue and green light onto a black background.\n",
    "\n",
    "There are so many color spaces because different color spaces are useful for different purposes. We will typically work with 3 kinds of color models: blue-green-red (BGR), grayscale and hue-saturation-value (HSV).\n",
    "\n",
    "In reality, color is a continuous phenomenon, meaning that there are an infinite number of colors. Color spaces, however represent color through discrete structures (a fixed number of whole number integer values), which is acceptable since the human eye and perception are also limited. \n",
    "\n",
    "## RGB color space\n",
    "It is an additive colorspace where colors are obtained by a linear combination of Red, Green and Blue values. \n",
    "\n",
    "There are some inherent problems asociated with RGB colorspace:\n",
    "- significant perceptual non-uniformity.\n",
    "- mixing of chrominance and luminance data.\n",
    "\n",
    "## LAB color space\n",
    "3 components:\n",
    "- L: lightness (intensity).\n",
    "- A: color component ranging from green to red.\n",
    "- B: color component ranging from blue to yellow.\n",
    "\n",
    "The L component is independent of color information and encodes brightness only. \n",
    "\n",
    "## YCrCb color space\n",
    "Like LAB colorspace, the luminance and chrominances are separated into different channels. Y is used to represent luminance (or luma), Cb represents blue-difference, and Cr represents red-difference.\n",
    "\n",
    "## HSV color space\n",
    "1. Hue is the color attribute that describes pure color. \n",
    "2. Saturation is the quantity that reflect the degree to which pure color is diluted by white light.\n",
    "3. Value or intensity is brightness.\n",
    "\n",
    "![hsv cylindrical spectrum](img_embed/hsv_cylindrical.jfif \"HSV\")\n",
    "\n",
    "> **Important** 🗝️\n",
    ">\n",
    "> In OpenCV, hue range is $[0,179]$, saturation range is $[0,255]$, and value range is $[0,255]$. Different software use different scales. So if you are comparing OpenCV values with them, you need to **normalize these ranges**. \n",
    "> The important takeaway is: *Lighting condition can mean the difference between success and failure of your computer vision algorithm.* Thus, color space which could factor out luminance into different channel should be the choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9bc8a-4feb-402e-88b2-20c4d0a89139",
   "metadata": {},
   "source": [
    "## Splitting and merging Image channels\n",
    "The B, G and R channels can be split into their individual planes when needed. Then the individual channels can be merged back together to form BGR image again. The splitting and merging operations can be attained by the following functions respectively:\n",
    "* `cv.split(m)`, where m is a multi-channel array.\n",
    "* `cv.merge(mv)`, where mv is a tuple / list of matrices to be merged; all the matrices in mv must have the same size and the same depth (precision of each pixel)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e2e4ca",
   "metadata": {},
   "source": [
    "# manipulate hsv channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc3f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv.imread(\"images/car.jpg\")\n",
    "#change BGR to HSV\n",
    "img_hsv = cv.cvtColor(img,cv.COLOR_BGR2HSV)\n",
    "#split the channles\n",
    "h,s,v = cv.split(img_hsv)\n",
    "\n",
    "#set a constant for H channel[0-179],it is not 0 -360 because opencv has normalized it\n",
    "h_bias = 70\n",
    "h_new = np.zeros_like(h) + h_bias\n",
    "h_new = np.uint8(h_new)\n",
    "\n",
    "transform = cv.merge((h_new, s, v))\n",
    "transform_display = cv.cvtColor(transform, cv.COLOR_HSV2BGR)\n",
    "\n",
    "from utils import display_images\n",
    "display_images([img,transform_display],(\"original\",f\"Hue = {h_bias}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0e6398",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saturation\n",
    "\n",
    "h,s,v = cv.split(img_hsv)\n",
    "\n",
    "#set a constant for s chanel([0-255])\n",
    "s_bias = 75\n",
    "s_new = np.zeros_like(h) + s_bias\n",
    "s_new = np.uint8(h_new)\n",
    "\n",
    "transform = cv.merge((h, s_new, v))\n",
    "transform_display = cv.cvtColor(transform, cv.COLOR_HSV2BGR)\n",
    "\n",
    "from utils import display_images\n",
    "display_images([img,transform_display],(\"original\",f\"Saturation = {s_bias}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a0da871-8964-413b-83ba-d47f1d86b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are the two images the same? True\n"
     ]
    }
   ],
   "source": [
    "b, g, r = cv.split(img)\n",
    "img_merge = cv.merge((b, g, r))\n",
    "\n",
    "print(f\"Are the two images the same? {np.equal(img, img_merge).all()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1625f56-7bd9-40b3-9d48-b6b579bcee5c",
   "metadata": {},
   "source": [
    "## Manipulate image channels\n",
    "\n",
    "### HSV color channels\n",
    "\n",
    "#### Hue channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4fbfda1-b4a9-4556-a5e1-b3d9ce5af446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hue to a certain value\n",
    "img = cv.imread(\"images/meal.jpg\")\n",
    "img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "h, s, v = cv.split(img_hsv)\n",
    "h_new = np.zeros_like(h) + 30\n",
    "h_new = np.uint8(h_new)\n",
    "\n",
    "transform = cv.merge((h_new, s, v))\n",
    "transform_display = cv.cvtColor(transform, cv.COLOR_HSV2BGR)\n",
    "\n",
    "display_images([img, transform_display], (\"original\", \"hue=30\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aba3ce6-74c4-4f0d-a783-84eb91e50268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the saturation \n",
    "img = cv.imread(\"images/meal.jpg\")\n",
    "img_hsv = cv.cvtColor(img, cv.COLOR_BGR2HSV)\n",
    "\n",
    "h, s, v = cv.split(img_hsv)\n",
    "s_new = np.zeros_like(s) + 255\n",
    "s_new = np.uint8(s_new)\n",
    "\n",
    "transform = cv.merge((h, s_new, v))\n",
    "transform_display = cv.cvtColor(transform, cv.COLOR_HSV2BGR)\n",
    "\n",
    "display_images([img, transform_display], (\"original\", \"saturation decrease\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46bfbe4-83a3-46aa-87d3-3dfd04769708",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "1. Display the blue, green and red channel of the **lena** image simultaneously. Comment on the images displayed.\n",
    "2. Display the following images which originates from the file \"images/dog.jfif\".\n",
    "\n",
    "![exercise_fig](img_embed/diff_color_channels.jpg \"color dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c85feb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 1\n",
    "\n",
    "img = cv.imread('images/lena.jfif')\n",
    "\n",
    "#blue = img[:,:,0]\n",
    "#green = img[:,:,1]\n",
    "#red = img[:,:,2]\n",
    "\n",
    "blue,green,red= cv.split(img)\n",
    "\n",
    "display_images([blue,green, red],(\"blue\",\"green\",\"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbd93af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 2\n",
    "img = cv.imread(\"images/dog.jfif\")\n",
    "img_copy = img.copy()\n",
    "\n",
    "colors = (\"blue\", \"green\", \"red\")\n",
    "h,s,v = cv.split(img)\n",
    "\n",
    "for i, channel in enumerate(channels):\n",
    "    new_channel = np.zeros_like(h) + 255\n",
    "    new_channel = np.uint8(new_channel)\n",
    "    img_copy = img.copy()\n",
    "    img_copy[...,i] = new_channel\n",
    "    cv.imshow(colors[i], img_copy)\n",
    "\n",
    "cv.waitKey(0)\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0004d398-06ec-49e1-a937-c4f41e8c2be5",
   "metadata": {},
   "source": [
    "# Point operator \n",
    "\n",
    "## Point operator\n",
    "It is merely mathematical operations on each pixel value of an image. The formula is as follows:\n",
    "$$f(x, y) = \\alpha f(x, y) + \\beta$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996ae9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.uint8([-1,0,255,256,257])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a821eb2-450b-4b3c-aa4a-f01632897bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets define the point operator function (multiplication, addition, subtraction and division)\n",
    "def point_op(img, alpha, beta):\n",
    "    \"\"\"Point operator function\n",
    "    Argument:\n",
    "    ---\n",
    "    img: input image\n",
    "    alpha: coefficient\n",
    "    beta: bias\n",
    "    \n",
    "    Returns:\n",
    "    ---\n",
    "    Unsigned 8-bit image array\"\"\"\n",
    "    img = img.astype(np.float32)\n",
    "    res = alpha * img + beta\n",
    "    # clip the pixel values \n",
    "    res = np.clip(res, 0, 255)\n",
    "    return np.uint8(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4d31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load on image to change its brightness\n",
    "\n",
    "img = cv.imread(\"img/camera.jpg\")\n",
    "alpha = 1.2 # increase constrast(>1), vice versa\n",
    "beta = 30 # increase brightness(>0), vice versa\n",
    "dst = poitn_op(img,alpha = alpha, beta = beta)\n",
    "\n",
    "display_images ([img_dst],(\"orginal\",\"pixel_transform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a5442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also do on others such as l on cielab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c652846-620d-4532-8083-8037cbc1423d",
   "metadata": {},
   "source": [
    "### Enhance the contrast and brighten the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27c9d436-468a-4708-89fb-d5a2c90c0912",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv.imread('images/camera.jpg', 0)\n",
    "transform = point_op(gray, 2, 0)\n",
    "\n",
    "display_images([gray, transform], (\"grayscale\", \"transform\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f568022f-0c59-4d12-afc8-372c2ff8a90b",
   "metadata": {},
   "source": [
    "### Lower the contrast and darken the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fdf6bbad-80c3-4e0c-a514-08e56e197ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform2 = point_op(gray, 1, -50)\n",
    "\n",
    "display_images([gray, transform2], (\"grayscale\", \"darken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192496f1-9be0-4956-a8aa-7cc1e0935779",
   "metadata": {},
   "source": [
    "## Weekly activity\n",
    "1. Create a **random noise color and grayscale** image. You can set your own width and height, but keep the total number of pixels of both images identical.\n",
    "2. Convert the code chunk found under section <a href=\"#Section1\">Divide an image into smaller patches using cropping</a> into a function with the following signature:\n",
    "```python\n",
    "crop_grid(img, num_horizontal_grid, num_vertical_grid, line_color)\n",
    " # img is the source image\n",
    " # num_horizontal_grid and num_vertical_grid are the number of patches along x and y axes.\n",
    " # line_color is the color of the grid line.\n",
    " # The output of the function should be image with grids\n",
    "```\n",
    "3. How would you *change the brightness* of a **color image**? Suggest **two ways** to perform the image processing operations. Implement your methods by providing the example codes. You are free to choose any image.\n",
    "4. Provide at least one common use cases for the following color spaces:\n",
    "    - RGB\n",
    "    - HSV\n",
    "    - CIELAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ef5805-dc0d-4964-9f8e-4c133a94ed01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 1\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Define image dimensions (adjust width and height as desired)\n",
    "width = 320\n",
    "height = 240\n",
    "\n",
    "# Calculate total number of pixels\n",
    "total_pixels = width * height\n",
    "\n",
    "# Create random color noise image\n",
    "color_noise = np.random.randint(0, 256, size=(height, width, 3), dtype=np.uint8)  # 3 channels for RGB\n",
    "\n",
    "# Create random grayscale noise image\n",
    "grayscale_noise = np.random.randint(0, 256, size=(total_pixels // 3, ), dtype=np.uint8)  # Integer division for grayscale\n",
    "\n",
    "# Reshape grayscale noise to match color image dimensions (repeat each value 3 times)\n",
    "grayscale_noise_reshaped = np.repeat(grayscale_noise, 3).reshape(height, width)\n",
    "\n",
    "# Display both images\n",
    "cv2.imshow(\"Color Noise\", color_noise)\n",
    "cv2.imshow(\"Grayscale Noise\", grayscale_noise_reshaped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a73b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 2\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def crop_grid(img, num_horizontal_grid, num_vertical_grid, line_color=(0, 255, 0)):\n",
    "  \"\"\"\n",
    "  Crops an image into a grid and displays grid lines.\n",
    "\n",
    "  Args:\n",
    "      img (np.ndarray): The source image as a NumPy array.\n",
    "      num_horizontal_grid (int): Number of patches along the horizontal axis.\n",
    "      num_vertical_grid (int): Number of patches along the vertical axis.\n",
    "      line_color (tuple, optional): Color of the grid lines in BGR format. Defaults to green (0, 255, 0).\n",
    "\n",
    "  Returns:\n",
    "      np.ndarray: The image with grid lines overlaid.\n",
    "  \"\"\"\n",
    "\n",
    "  height, width = img.shape[:2]  # Get image height and width\n",
    "\n",
    "  # Calculate patch dimensions (consider integer division for floor)\n",
    "  patch_width = width // num_horizontal_grid\n",
    "  patch_height = height // num_vertical_grid\n",
    "\n",
    "  # Draw horizontal grid lines\n",
    "  for i in range(1, num_vertical_grid):\n",
    "    y_pos = i * patch_height\n",
    "    cv2.line(img, (0, y_pos), (width, y_pos), line_color, thickness=2)\n",
    "\n",
    "  # Draw vertical grid lines\n",
    "  for i in range(1, num_horizontal_grid):\n",
    "    x_pos = i * patch_width\n",
    "    cv2.line(img, (x_pos, 0), (x_pos, height), line_color, thickness=2)\n",
    "\n",
    "  return img\n",
    "\n",
    "\n",
    "# Example usage\n",
    "img = cv2.imread(\"images/winter.jfif\")\n",
    "grid_img = crop_grid(img.copy(), 3, 2)  # Copy image to avoid modifying original\n",
    "cv2.imshow(\"Image with Grid\", grid_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d205bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3\n",
    "\n",
    "#1st way: using cv2.addWeighted\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def adjust_brightness_weighted(image, brightness_factor):\n",
    "  \"\"\"\n",
    "  Adjusts the brightness of a color image using cv2.addWeighted.\n",
    "\n",
    "  Args:\n",
    "      image (np.ndarray): The input image as a NumPy array.\n",
    "      brightness_factor (float): A value to add to each pixel (positive for brighter, negative for darker).\n",
    "\n",
    "  Returns:\n",
    "      np.ndarray: The image with adjusted brightness.\n",
    "  \"\"\"\n",
    "\n",
    "  alpha = 1.0  # Controls the overall weight of the image\n",
    "  beta = brightness_factor  # Value to add to each pixel\n",
    "\n",
    "  # Convert image to float32 for better precision during calculations\n",
    "  image = image.astype(np.float32)\n",
    "\n",
    "  # Apply weighting to adjust brightness\n",
    "  adjusted_image = cv2.addWeighted(image, alpha, image, 0, beta)\n",
    "\n",
    "  # Convert back to uint8 for display\n",
    "  adjusted_image = adjusted_image.astype(np.uint8)\n",
    "\n",
    "  return adjusted_image\n",
    "\n",
    "# Example usage\n",
    "image = cv2.imread(\"images/soccer.jpg\")\n",
    "brighter_image = adjust_brightness_weighted(image.copy(), 30)  # Increase brightness by 30\n",
    "darker_image = adjust_brightness_weighted(image.copy(), -20)  # Decrease brightness by 20\n",
    "\n",
    "# Display original and adjusted images\n",
    "cv2.imshow(\"Original Image\", image)\n",
    "cv2.imshow(\"Brighter Image\", brighter_image)\n",
    "cv2.imshow(\"Darker Image\", darker_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61f80f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question 3\n",
    "\n",
    "#2nd way: using channel scalling\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def adjust_brightness_scaling(image, brightness_factor):\n",
    "  \"\"\"\n",
    "  Adjusts the brightness of a color image by scaling channel intensities.\n",
    "\n",
    "  Args:\n",
    "      image (np.ndarray): The input image as a NumPy array.\n",
    "      brightness_factor (float): A scaling factor for pixel intensities (positive for brighter, negative for darker).\n",
    "\n",
    "  Returns:\n",
    "      np.ndarray: The image with adjusted brightness.\n",
    "  \"\"\"\n",
    "\n",
    "  # Check if brightness factor is within a reasonable range\n",
    "  if brightness_factor < -1.0 or brightness_factor > 1.0:\n",
    "    raise ValueError(\"Brightness factor must be between -1.0 and 1.0\")\n",
    "\n",
    "  # Apply scaling to each channel using broadcasting\n",
    "  adjusted_image = np.clip((image * (1.0 + brightness_factor)), 0, 255).astype(np.uint8)\n",
    "\n",
    "  return adjusted_image\n",
    "\n",
    "# Example usage (same as method 1)\n",
    "image = cv2.imread(\"images/soccer.jpg\")\n",
    "brighter_image = adjust_brightness_scaling(image.copy(), 0.2)  # Increase brightness by 20%\n",
    "darker_image = adjust_brightness_scaling(image.copy(), -0.1)  # Decrease brightness by 10%\n",
    "\n",
    "# Display original and adjusted images (same as method 1)\n",
    "cv2.imshow(\"Original Image\", image)\n",
    "cv2.imshow(\"Brighter Image\", brighter_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76034f17",
   "metadata": {},
   "source": [
    "question 4:\n",
    "\n",
    "    RGB:Digital images, displays, image processing\n",
    "    HSV:Color selection, manipulation, image segmentation\n",
    "    CIELAB:Color matching, difference measurement, color gamut mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
